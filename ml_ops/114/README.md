# SLURM を使用して複数サーバーでの分散学習を行う

## 方法（Ubuntu や Debian の場合）

1. 複数サーバーを用意する<br>
    マスターノードのサーバー A とワーカーノードのサーバー B を用意する

1. サーバーのホスト名を設定する<br>
    `slurm.conf` で定義するホスト名 `SlurmctldHost`, `NodeName` の名前解決を行うため、両方のサーバーの `/etc/hosts` をそれぞれ編集する
    ```bash
    sudo vi /etc/hosts
    ```

    `/etc/hosts` に以下の行を追加する
    ```
    <サーバー A のIPアドレス（内部IP）>    server-a（サーバーAのホスト名）
    <サーバー B のIPアドレス（内部IP）>    server-b（サーバーBのホスト名）
    ```

1. 両方のサーバーのポートを開放する<br>
    ファイアウォールで `slurm.conf` で定義したポート `SlurmctldPort` を開放し、相互に通信できるようにする

    - GCE の場合<br>
        1. VPCネットワーク → ファイアウォール → ファイアウォールルールを作成
            - 名前: `allow-slurm-cluster`
            - ターゲットタグ: `slurm-cluster`（または適切なタグ）
            - ソースIPの範囲: `xx.xxx.0.0/16`（サーバー A と サーバー B の内部IPの範囲になるように指定）
            - プロトコルとポート:
                - `tcp:xxxx-xxxx`（`slurm.conf` で定義したポート番号になるように指定）
                - その他: `icmp`（ping で疎通確認できるようにする）

        1. 両方の GCE インスタンス（サーバーA & サーバーB）のネットワークタグに `slurm-cluster` を追加する

1. （オプション）ping で疎通確認を行う<br>
    ```bash
    ping server-a
    ping server-b
    ```

1. 複数サーバーに SLURM をインストールする<br>
    [SLURM をインストールする](https://github.com/Yagami360/ai-product-dev-tips/tree/master/ml_ops/112)　に従って、サーバー A には、`slurmctld` と `slurmd` をインストールし、サーバー B には、`slurmd` をインストールする

1. （オプション）複数サーバーの slurm 用ユーザー `slurm` の ID を一致させる<br>
    サーバー A と サーバー B の `slurm` 用ユーザーの ID が異なる場合は、同じ ID になるように変更する
    `slurm.conf` で定義した `SlurmUser` の ID を一致させる
    ```bash
    # USER ID を確認する
    id slurm
    ```

    例えば、USER ID が 5000 の場合は、以下のように設定する
    ```bash
    sudo usermod -u 5000 slurm
    sudo groupmod -g 5000 slurm
    ```

1. 複数サーバーの `slurm.conf` を設定する<br>
    両方のサーバーの `slurm.conf` を設定する

    - サーバー A の `slurm.conf`<br>
        ```conf
        # slurm.conf file generated by configurator easy.html.
        # Put this file on all nodes of your cluster.
        # See the slurm.conf man page for more information.
        #
        ClusterName=cluster
        SlurmctldHost=server-a      # マスターノード（サーバーA）のホスト名
        #
        #MailProg=/bin/mail
        MpiDefault=none
        #MpiParams=ports=#-#
        ProctrackType=proctrack/cgroup
        # ProctrackType=proctrack/linuxproc
        ReturnToService=1
        SlurmctldPidFile=/var/run/slurmctld.pid
        #SlurmctldPort=6817
        SlurmdPidFile=/var/run/slurmd.pid
        #SlurmdPort=6818
        SlurmdSpoolDir=/var/spool/slurmd
        SlurmUser=slurm
        #SlurmdUser=root
        StateSaveLocation=/var/spool/slurmctld
        SwitchType=switch/none
        TaskPlugin=task/affinity,task/cgroup
        #
        #
        # TIMERS
        #KillWait=30
        #MinJobAge=300
        #SlurmctldTimeout=120
        #SlurmdTimeout=300
        #
        #
        # SCHEDULING
        SchedulerType=sched/backfill
        SelectType=select/cons_tres
        #
        #
        # LOGGING AND ACCOUNTING
        AccountingStorageType=accounting_storage/none
        #JobAcctGatherFrequency=30
        JobAcctGatherType=jobacct_gather/none
        #SlurmctldDebug=info
        SlurmctldLogFile=/var/log/slurmctld.log
        #SlurmdDebug=info
        SlurmdLogFile=/var/log/slurmd.log
        #
        #
        # COMPUTE NODES（'/usr/local/sbin/slurmd -C' で確認可能）
        NodeName=server-a CPUs=4 State=UNKNOWN
        NodeName=server-b CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=15033
        PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP
        ```

    - サーバー B の `slurm.conf`<br>
        ```conf
        # slurm.conf file generated by configurator easy.html.
        # Put this file on all nodes of your cluster.
        # See the slurm.conf man page for more information.
        #
        ClusterName=cluster
        SlurmctldHost=server-a      # マスターノード（サーバーA）のホスト名
        #
        #MailProg=/bin/mail
        MpiDefault=none
        #MpiParams=ports=#-#
        ProctrackType=proctrack/cgroup
        # ProctrackType=proctrack/linuxproc
        ReturnToService=1
        SlurmctldPidFile=/var/run/slurmctld.pid
        #SlurmctldPort=6817
        SlurmdPidFile=/var/run/slurmd.pid
        #SlurmdPort=6818
        SlurmdSpoolDir=/var/spool/slurmd
        SlurmUser=slurm
        #SlurmdUser=root
        StateSaveLocation=/var/spool/slurmctld
        SwitchType=switch/none
        TaskPlugin=task/affinity,task/cgroup
        #
        #
        # TIMERS
        #KillWait=30
        #MinJobAge=300
        #SlurmctldTimeout=120
        #SlurmdTimeout=300
        #
        #
        # SCHEDULING
        SchedulerType=sched/backfill
        SelectType=select/cons_tres
        #
        #
        # LOGGING AND ACCOUNTING
        AccountingStorageType=accounting_storage/none
        #JobAcctGatherFrequency=30
        JobAcctGatherType=jobacct_gather/none
        #SlurmctldDebug=info
        SlurmctldLogFile=/var/log/slurmctld.log
        #SlurmdDebug=info
        SlurmdLogFile=/var/log/slurmd.log
        #
        #
        # COMPUTE NODES（'/usr/local/sbin/slurmd -C' で確認可能）
        NodeName=server-a CPUs=4 State=UNKNOWN
        NodeName=server-b CPUs=4 Boards=1 SocketsPerBoard=1 CoresPerSocket=2 ThreadsPerCore=2 RealMemory=15033
        PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP
        ```

1. 複数サーバーの MUGE キーを一致させる<br>
    ```bash
    sudo cat /etc/munge/munge.key
    ```

    サーバー A と サーバー B の `munge.key` の内容が一致していることを確認する。
    一致していない場合は、サーバー A の `munge.key` をサーバー B にコピーする
    ```bash
    # サーバー A で実行
    sudo scp /etc/munge/munge.key username@server-b:/tmp/munge.key

    # サーバー B で実行
    sudo mv /tmp/munge.key /etc/munge/munge.key
    sudo chown munge:munge /etc/munge/munge.key
    sudo chmod 400 /etc/munge/munge.key
    ```

    その後、両方のサーバーで MUNGE を再起動する
    ```bash
    sudo systemctl restart munge
    ```

1. 複数サーバーの `slurmctld` と `slurmd` を起動する<br>

    - サーバー A : `slurmctld` と `slurmd` を起動する
        ```bash
        sudo systemctl daemon-reload
        sudo systemctl start slurmctld slurmd
        ```

    - サーバー B : `slurmd` のみを起動する
        ```bash
        sudo systemctl daemon-reload
        sudo systemctl start slurmd
        ```

1. ノードの状態を確認する<br>
    ```bash
    sinfo
    ```
    ```
    PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
    debug*       up   infinite      2   idle server-a,server-b
    ```

    ```bash
    scontrol show node
    ```

1. 学習用ジョブのスクリプトを作成する<br>

1. マスターノード（サーバーA）からワーカーノード（サーバーB）にジョブを投入する<br>

    ```bash
    srun --nodelist=server-b --ntasks=1 --ntasks-per-node=1 python train.py
    ```

    GPU の分散学習を行う場合は、`--gres=gpu:1` オプションを追加する
    ```bash
    srun --nodes=2 --ntasks=2 --ntasks-per-node=1 --gres=gpu:1 python train.py
    ```
