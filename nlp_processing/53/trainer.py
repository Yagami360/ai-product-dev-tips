import json
import os
from datetime import datetime
from typing import Any, Dict, Optional, Union

import torch
import torch.nn.functional as F
from transformers import Trainer


class LogitDistillationTrainer(Trainer):
    def __init__(self, teacher_model, temperature=2.0, alpha=0.5, tokenizer=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.teacher = teacher_model
        self.teacher.eval()
        self.temperature = temperature
        self.alpha = alpha
        self.tokenizer = tokenizer
        self.debug_step = 0
        for param in self.teacher.parameters():
            param.requires_grad = False

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        # [compute_loss] inputs: {
        #     'input_ids': tensor([[    48,     25,  32016,  ..., 151643, 151643, 151643],
        #         [    48,     25,  10978,  ..., 151643, 151643, 151643],
        #         [    48,     25,   2619,  ..., 151643, 151643, 151643],
        #         [    48,     25,  43924,  ..., 151643, 151643, 151643]],
        #        device='cuda:0'),
        #   'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        #         [1, 1, 1,  ..., 0, 0, 0],
        #         [1, 1, 1,  ..., 0, 0, 0],
        #         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),
        #   'labels': tensor([[   48,    25, 32016,  ...,  -100,  -100,  -100],
        #         [   48,    25, 10978,  ...,  -100,  -100,  -100],
        #         [   48,    25,  2619,  ...,  -100,  -100,  -100],
        #         [   48,    25, 43924,  ...,  -100,  -100,  -100]], device='cuda:0')}
        # print("[compute_loss] inputs:", inputs)

        # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–çµæœï¼ˆæ­£è§£ãƒ‡ãƒ¼ã‚¿ï¼‰
        with torch.no_grad():
            outputs_teacher = self.teacher(**inputs)

            # æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            if self.debug_step == 0 and self.tokenizer is not None:
                print("\n" + "=" * 80)
                print("ğŸ” æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–çµæœã‚’ç¢ºèªï¼ˆæœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰")
                print("=" * 80)

                input_ids = inputs["input_ids"][0]
                attention_mask = inputs["attention_mask"][0]
                labels = inputs["labels"][0]

                # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é™¤å¤–ï¼‰
                valid_input_ids = input_ids[attention_mask == 1]
                input_text = self.tokenizer.decode(valid_input_ids, skip_special_tokens=True)
                print(f"\nğŸ“ å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ:\n{input_text}\n")

                # æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆ-100ã‚’é™¤å¤–ï¼‰
                valid_labels = labels[labels != -100]
                if len(valid_labels) > 0:
                    label_text = self.tokenizer.decode(valid_labels, skip_special_tokens=True)
                    print(f"âœ… æ­£è§£ï¼ˆå­¦ç¿’å¯¾è±¡ï¼‰:\n{label_text}\n")

                # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ï¼ˆè§£ç­”éƒ¨åˆ†ï¼‰
                answer_start_idx = (labels != -100).nonzero(as_tuple=True)[0][0].item() if len(valid_labels) > 0 else 0
                if answer_start_idx > 0:
                    teacher_answer_logits = outputs_teacher.logits[0][answer_start_idx - 1 : attention_mask.sum() - 1]
                    teacher_answer_predictions = teacher_answer_logits.argmax(dim=-1)
                    predicted_text = self.tokenizer.decode(teacher_answer_predictions, skip_special_tokens=True)
                    print(f"ğŸ“ æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬:\n{predicted_text}\n")

                print("=" * 80 + "\n")
                self.debug_step += 1

        # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–çµæœï¼ˆäºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ï¼‰
        outputs_student = model(**inputs)

        # Cross Entropy lossã€€(hard target loss)
        # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿é–“ã® lossã€‚ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãŒæ­£è§£ãƒ‡ãƒ¼ã‚¿ã«è¿‘ã¥ãã‚ˆã†ã«ã™ã‚‹
        # Hugging Face ã® transformers ã® AutoModelForCausalLM ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã™éš› (model(**inputs)) ã«ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (inputs) ã« labels (æ­£è§£ãƒ‡ãƒ¼ã‚¿) ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã¯å†…éƒ¨ã§ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤± (Cross Entropy Loss) ã‚’è‡ªå‹•çš„ã«è¨ˆç®—ã—ã€.loss å±æ€§ã«æ ¼ç´
        loss_ce = outputs_student.loss

        # KL Divergence loss (soft target loss)
        # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ç¢ºç‡åˆ†å¸ƒã¨ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®ç¢ºç‡åˆ†å¸ƒé–“ã®è·é›¢ã‚’æœ€å°åŒ–ã—ã€ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãŒæ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã«è¿‘ã¥ãã‚ˆã†ã«ã™ã‚‹
        # loss_kd = F.kl_div(
        #     F.log_softmax(outputs_student.logits / self.temperature, dim=-1),
        #     F.softmax(outputs_teacher.logits / self.temperature, dim=-1),
        #     reduction="batchmean",
        # ) * (self.temperature**2)

        # KL Divergence ã‚’è¨ˆç®—ï¼ˆreduction="none" ã§å„è¦ç´ ã”ã¨ã® loss ã‚’å–å¾—ï¼‰
        student_logits = outputs_student.logits / self.temperature
        teacher_logits = outputs_teacher.logits / self.temperature
        loss_kd = F.kl_div(
            F.log_softmax(student_logits, dim=-1),
            F.softmax(teacher_logits, dim=-1),
            reduction="none",
        ).sum(dim=-1)

        # attention_mask ã‚’é©ç”¨ã—ã¦æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã®å¹³å‡ã‚’å–ã‚‹ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨ˆç®—å¯¾è±¡ã‹ã‚‰é™¤å¤–ï¼‰
        attention_mask = inputs.get("attention_mask", None)
        if attention_mask is not None:
            loss_kd = (loss_kd * attention_mask).sum() / attention_mask.sum()
        else:
            loss_kd = loss_kd.mean()

        loss_kd = loss_kd * (self.temperature**2)

        # Total loss: Cross Entropy loss ã¨ KL Divergence loss ã®ç·šå½¢çµåˆ
        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kd
        return (loss, outputs_student) if return_outputs else loss
