"""
Utility functions for monitoring memory usage
"""

from typing import Optional

import torch


def get_model_memory_usage(model: torch.nn.Module) -> dict:
    """
    ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨ˆç®—

    Args:
        model: PyTorchãƒ¢ãƒ‡ãƒ«

    Returns:
        dict: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æƒ…å ±
            - params_mb: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
            - buffers_mb: ãƒãƒƒãƒ•ã‚¡ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
            - total_mb: åˆè¨ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
            - num_params: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
    """
    param_size = 0
    buffer_size = 0
    num_params = 0

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
    for param in model.parameters():
        num_params += param.numel()
        param_size += param.numel() * param.element_size()

    # ãƒãƒƒãƒ•ã‚¡ã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
    for buffer in model.buffers():
        buffer_size += buffer.numel() * buffer.element_size()

    # MBã«å¤‰æ›
    param_mb = param_size / 1024 / 1024
    buffer_mb = buffer_size / 1024 / 1024
    total_mb = param_mb + buffer_mb

    return {
        "params_mb": param_mb,
        "buffers_mb": buffer_mb,
        "total_mb": total_mb,
        "num_params": num_params,
    }


def print_model_memory(model: torch.nn.Module, model_name: str = "Model", detailed: bool = True):
    """
    ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º

    Args:
        model: PyTorchãƒ¢ãƒ‡ãƒ«
        model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆè¡¨ç¤ºç”¨ï¼‰
        detailed: è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã‹
    """
    info = get_model_memory_usage(model)

    print(f"\n{'='*60}")
    print(f"ğŸ“Š {model_name} - Memory Usage")
    print(f"{'='*60}")
    print(f"Parameters:     {info['num_params']:,} ({info['num_params']/1e6:.2f}M)")
    print(f"Memory (Params): {info['params_mb']:.2f} MB ({info['params_mb']/1024:.2f} GB)")

    if detailed:
        print(f"Memory (Buffers): {info['buffers_mb']:.2f} MB")
        print(f"Total Memory:    {info['total_mb']:.2f} MB ({info['total_mb']/1024:.2f} GB)")

    print(f"{'='*60}")


def get_gpu_memory_usage() -> Optional[dict]:
    """
    GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—

    Returns:
        dict or None: GPU ãƒ¡ãƒ¢ãƒªæƒ…å ±ï¼ˆGPUãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯Noneï¼‰
            - allocated_mb: å‰²ã‚Šå½“ã¦æ¸ˆã¿ãƒ¡ãƒ¢ãƒªï¼ˆMBï¼‰
            - reserved_mb: äºˆç´„æ¸ˆã¿ãƒ¡ãƒ¢ãƒªï¼ˆMBï¼‰
            - total_mb: ç·ãƒ¡ãƒ¢ãƒªï¼ˆMBï¼‰
    """
    if not torch.cuda.is_available():
        return None

    allocated = torch.cuda.memory_allocated() / 1024 / 1024
    reserved = torch.cuda.memory_reserved() / 1024 / 1024
    total = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024

    return {
        "allocated_mb": allocated,
        "reserved_mb": reserved,
        "total_mb": total,
    }


def print_gpu_memory(detailed: bool = True):
    """
    GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º

    Args:
        detailed: è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã‹
    """
    info = get_gpu_memory_usage()

    if info is None:
        print("\nâš ï¸  GPU is not available")
        return

    print(f"\n{'='*60}")
    print(f"ğŸ® GPU Memory Usage")
    print(f"{'='*60}")
    print(f"Allocated: {info['allocated_mb']:.2f} MB ({info['allocated_mb']/1024:.2f} GB)")
    print(f"Reserved:  {info['reserved_mb']:.2f} MB ({info['reserved_mb']/1024:.2f} GB)")

    if detailed:
        print(f"Total:     {info['total_mb']:.2f} MB ({info['total_mb']/1024:.2f} GB)")
        usage_percent = (info["allocated_mb"] / info["total_mb"]) * 100
        print(f"Usage:     {usage_percent:.1f}%")

    print(f"{'='*60}")


def print_memory_summary(teacher_model: Optional[torch.nn.Module] = None, student_model: Optional[torch.nn.Module] = None, show_gpu: bool = True):
    """
    æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã€ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã€GPUã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ã¾ã¨ã‚ã¦è¡¨ç¤º

    Args:
        teacher_model: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        student_model: ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        show_gpu: GPUãƒ¡ãƒ¢ãƒªã‚‚è¡¨ç¤ºã™ã‚‹ã‹
    """
    print("\n" + "=" * 60)
    print("ğŸ’¾ Memory Summary")
    print("=" * 60)

    if teacher_model is not None:
        teacher_info = get_model_memory_usage(teacher_model)
        print(f"\nğŸ‘¨â€ğŸ« Teacher Model:")
        print(f"  Parameters: {teacher_info['num_params']:,} ({teacher_info['num_params']/1e6:.2f}M)")
        print(f"  Memory:     {teacher_info['total_mb']:.2f} MB ({teacher_info['total_mb']/1024:.2f} GB)")

    if student_model is not None:
        student_info = get_model_memory_usage(student_model)
        print(f"\nğŸ‘¨â€ğŸ“ Student Model:")
        print(f"  Parameters: {student_info['num_params']:,} ({student_info['num_params']/1e6:.2f}M)")
        print(f"  Memory:     {student_info['total_mb']:.2f} MB ({student_info['total_mb']/1024:.2f} GB)")

    if teacher_model is not None and student_model is not None:
        ratio = teacher_info["num_params"] / student_info["num_params"]
        memory_ratio = teacher_info["total_mb"] / student_info["total_mb"]
        print(f"\nğŸ“Š Compression Ratio:")
        print(f"  Parameters: {ratio:.2f}x")
        print(f"  Memory:     {memory_ratio:.2f}x")

    if show_gpu:
        gpu_info = get_gpu_memory_usage()
        if gpu_info is not None:
            print(f"\nğŸ® GPU Memory:")
            print(f"  Allocated: {gpu_info['allocated_mb']:.2f} MB ({gpu_info['allocated_mb']/1024:.2f} GB)")
            print(f"  Reserved:  {gpu_info['reserved_mb']:.2f} MB ({gpu_info['reserved_mb']/1024:.2f} GB)")
            print(f"  Total:     {gpu_info['total_mb']:.2f} MB ({gpu_info['total_mb']/1024:.2f} GB)")
            usage_percent = (gpu_info["allocated_mb"] / gpu_info["total_mb"]) * 100
            print(f"  Usage:     {usage_percent:.1f}%")

    print("=" * 60 + "\n")
