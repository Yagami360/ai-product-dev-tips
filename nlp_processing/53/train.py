import argparse
import os

import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, default_data_collator

from trainer import LogitDistillationTrainer
from utils import print_gpu_memory, print_memory_summary, print_model_memory


def train(args):
    print("\n" + "=" * 60)
    print("Logit-based Knowledge Distillation")
    print("=" * 60)

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå˜èªãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼‰
    # æ•™å¸«ã®çŸ¥è­˜ã‚’æ­£ç¢ºã«è’¸ç•™ã™ã‚‹ãŸã‚ã«ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹
    print("\nğŸ“¥ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.teacher_model_name,
        trust_remote_code=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": "<|endoftext|>"})
    if tokenizer.eos_token is None:
        tokenizer.add_special_tokens({"eos_token": "<|im_end|>"})

    print(f"   Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    print(f"   EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")

    # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ï¼ˆè’¸ç•™å…ƒãƒ¢ãƒ‡ãƒ«ï¼‰
    print(f"ğŸ“¥ Loading teacher model: {args.teacher_model_name}")
    if args.use_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            # bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        teacher_model = AutoModelForCausalLM.from_pretrained(
            args.teacher_model_name,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype="auto",
            # torch_dtype=torch.float16,
            quantization_config=bnb_config,
        )
    else:
        teacher_model = AutoModelForCausalLM.from_pretrained(
            args.teacher_model_name,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype="auto",
            # torch_dtype=torch.float16,
        )

    # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ï¼ˆè’¸ç•™å…ˆãƒ¢ãƒ‡ãƒ«ï¼‰
    print(f"ğŸ“¥ Loading student model: {args.student_model_name}")
    student_model = AutoModelForCausalLM.from_pretrained(
        args.student_model_name,
        trust_remote_code=True,
        device_map="auto",
        torch_dtype="auto",
        # torch_dtype=torch.float16,
        # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã¯ã€4bit é‡å­åŒ–åˆ©ç”¨ä¸å¯
        # load_in_4bit=True if args.use_4bit else False,
    )

    # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚ºã‚’æ•™å¸«ãƒ¢ãƒ‡ãƒ«ï¼ˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼‰ã«åˆã‚ã›ã‚‹
    # è’¸ç•™æ™‚ã«ã¯ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«ã‚¢ãƒ³ãƒãƒƒãƒãŒç”Ÿã˜ã‚‹ãŸã‚å¿…è¦
    # èªå½™ã‚µã‚¤ã‚ºï¼ˆVocabulary Sizeï¼‰: ãƒ¢ãƒ‡ãƒ«ãŒæ‰±ãˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå˜èªã®æœ€å°å˜ä½ï¼‰ã®ç¨®é¡ã®æ•°
    if student_model.config.vocab_size != len(tokenizer):
        print(f"   âš ï¸  Student-model vocab size mismatch! Resizing...")
        print(f"   Resizing student: {student_model.config.vocab_size} -> {len(tokenizer)}")
        student_model.resize_token_embeddings(len(tokenizer))

    # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚ºã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«åˆã‚ã›ã‚‹
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ã—ã¦æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€é€šå¸¸ã¯ä¸è¦
    if teacher_model.config.vocab_size != len(tokenizer):
        print(f"   âš ï¸  Teacher-model vocab size mismatch! Resizing...")
        print(f"   Resizing teacher: {teacher_model.config.vocab_size} -> {len(tokenizer)}")
        teacher_model.resize_token_embeddings(len(tokenizer))

    print("\nğŸ” Validating vocab sizes after resizing...")
    print(f"   Tokenizer:  {len(tokenizer)}")
    print(f"   Teacher Model:    {teacher_model.config.vocab_size}")
    print(f"   Student Model:    {student_model.config.vocab_size}")

    # ãƒ¢ãƒ‡ãƒ«ã® pad_token_id ã¨ eos_token_id ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ä¸€è‡´ã•ã›ã‚‹
    teacher_model.config.pad_token_id = tokenizer.pad_token_id
    teacher_model.config.eos_token_id = tokenizer.eos_token_id
    student_model.config.pad_token_id = tokenizer.pad_token_id
    student_model.config.eos_token_id = tokenizer.eos_token_id

    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¡¨ç¤º
    print_model_memory(teacher_model, f"Teacher Model: {args.teacher_model_name}")
    print_model_memory(student_model, f"Student Model: {args.student_model_name}")
    print_memory_summary(teacher_model, student_model, show_gpu=True)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
    # GSM8K: ç®—æ•°ã®å•é¡Œã¨è§£ç­”ãŒå«ã¾ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    print(f"\nğŸ“Š Loading dataset: GSM8K")
    dataset = load_dataset("gsm8k", "main", split="train[:]")
    print(f"âœ… Dataset loaded: {len(dataset)} samples")

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›: question ã¨ answer ã‚’çµåˆã—ã¦ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨ã™ã‚‹
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã« answer ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹ã“ã¨ã«ãªã‚‹ã®ã§ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã«æ¨è«–ã•ã›ã‚‹éš›ã«æ­£è§£ã‚’æ•™ãˆã¦ã„ã‚‹ã“ã¨ã«ãªã‚‹ã®ã§ã¯ï¼Ÿã¨ã„ã†ç–‘å•ãŒç”Ÿã˜ã‚‹ãŒã€
    # è’¸ç•™ã«ãŠã‘ã‚‹æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²ãŒã€ã€Œæ­£è§£ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã€ã§ã¯ãªãã€ã€Œæ­£è§£ã‚’çŸ¥ã£ã¦ã„ã‚‹çŠ¶æ…‹ã§ã®æ¨è«–ã®ã€æŒ¯ã‚‹èˆã„ã€ã‚’æä¾›ã™ã‚‹ã“ã¨ã€ã«ã‚ã‚‹ã®ã§å•é¡Œãªã—ã€‚
    dataset = dataset.map(lambda x: {"text": f"Q: {x['question']}\nA: {x['answer']}"})
    # dataset[0]: {
    #     'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',
    #     'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n#### 72',
    #     'text': 'Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\nA: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n#### 72'
    # }
    print("[train] dataset[0]:", dataset[0])

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–: å˜èªãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›ã™ã‚‹
    print("   Tokenizing dataset...")

    def tokenize_function(examples):
        # text ã® QA éƒ¨åˆ†ã ã‘ä½¿ç”¨ã™ã‚‹
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            max_length=args.tokenizer_max_length,
            padding="max_length",
            return_tensors=None,
            add_special_tokens=True,
        )

        # labelsã‚’è¿½åŠ 
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ©ãƒ™ãƒ«ã‚’ -100 ã«è¨­å®šã—ã¦ã€æå¤±è¨ˆç®—ã‹ã‚‰é™¤å¤–ã™ã‚‹
        # ã¾ãŸã€è³ªå•éƒ¨åˆ†ï¼ˆ"Q: ... A:" ã¾ã§ï¼‰ã‚‚ -100 ã«è¨­å®šã—ã¦ã€è§£ç­”éƒ¨åˆ†ã®ã¿ã‚’å­¦ç¿’å¯¾è±¡ã«ã™ã‚‹
        labels = []
        for i, input_ids in enumerate(tokenized["input_ids"]):
            # è³ªå•ã¨è§£ç­”ã®å¢ƒç•Œã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆ"A:" ã®å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰å­¦ç¿’å¯¾è±¡ï¼‰
            # "A:" ã¯é€šå¸¸ "\nA:" ã¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã‚‹
            text = examples["text"][i]
            question_answer_split = text.split("\nA:", 1)

            # è³ªå•éƒ¨åˆ†ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
            if len(question_answer_split) == 2:
                question_part = question_answer_split[0] + "\nA:"
                question_tokens = tokenizer(question_part, truncation=False, add_special_tokens=True)["input_ids"]
                question_length = len(question_tokens)
            else:
                question_length = 0

            # labels ã‚’ä½œæˆï¼šè³ªå•éƒ¨åˆ†ã¨ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã‚’ -100 ã«è¨­å®š
            label = []
            for j, token_id in enumerate(input_ids):
                if j < question_length:
                    # è³ªå•éƒ¨åˆ†ã¯å­¦ç¿’å¯¾è±¡å¤–ï¼ˆlosså€¤ã®è¨ˆç®—å¯¾è±¡å¤–ï¼‰
                    label.append(-100)
                elif token_id == tokenizer.pad_token_id:
                    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã‚‚å­¦ç¿’å¯¾è±¡å¤–ï¼ˆlosså€¤ã®è¨ˆç®—å¯¾è±¡å¤–ï¼‰
                    label.append(-100)
                else:
                    # è§£ç­”éƒ¨åˆ†ã®ã¿å­¦ç¿’å¯¾è±¡
                    label.append(token_id)
            labels.append(label)
        tokenized["labels"] = labels
        return tokenized

    train_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)
    print(f"âœ… Dataset ready: {len(train_dataset)} samples")

    # input_ids ã« `text` ã®éƒ¨åˆ†ã®QAãƒ†ã‚­ã‚¹ãƒˆãŒå…¥ã£ã¦ã„ã‚‹
    # train_dataset[0]: {
    #     'input_ids': [48, 25, 41601, 685, 6088, 26111, 311, 220, 19, 23, 315, 1059, 4780, 304, 5813, 11, 323, 1221, 1340, 6088, 4279, 438, 1657, 26111, 304, 3217, 13, 2585, 1657, 26111, 1521, 41601, 685, 4559, 30055, 304, 5813, 323, 3217, 5267, 32, 25, 41601, 685, 6088, 220, 19, 23, 14, 17, 284, 1115, 19, 23, 14, 17, 28, 17, 19, 2452, 17, 19, 26111, 304, 3217, 624, 45, 4212, 685, 6088, 220, 19, 23, 10, 17, 19, 284, 1115, 19, 23, 10, 17, 19, 28, 22, 17, 2452, 22, 17, 26111, 30055, 304, 5813, 323, 3217, 624, 820, 220, 22, 17, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],
    #     'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    #     'labels': [48, 25, 41601, 685, 6088, 26111, 311, 220, 19, 23, 315, 1059, 4780, 304, 5813, 11, 323, 1221, 1340, 6088, 4279, 438, 1657, 26111, 304, 3217, 13, 2585, 1657, 26111, 1521, 41601, 685, 4559, 30055, 304, 5813, 323, 3217, 5267, 32, 25, 41601, 685, 6088, 220, 19, 23, 14, 17, 284, 1115, 19, 23, 14, 17, 28, 17, 19, 2452, 17, 19, 26111, 304, 3217, 624, 45, 4212, 685, 6088, 220, 19, 23, 10, 17, 19, 284, 1115, 19, 23, 10, 17, 19, 28, 22, 17, 2452, 22, 17, 26111, 30055, 304, 5813, 323, 3217, 624, 820, 220, 22, 17, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}
    print("train_dataset[0]:", train_dataset[0])

    # Training Arguments
    print("\nâš™ï¸  Setting up training arguments...")
    training_args = TrainingArguments(
        output_dir=f"{args.output_dir}/{args.exper_name}",
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        logging_steps=args.logging_steps,
        logging_dir=f"{args.output_dir}/{args.exper_name}/logs",
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        learning_rate=args.learning_rate,
        optim=args.optimizer,
        dataloader_pin_memory=True if torch.cuda.is_available() else False,
        remove_unused_columns=False,
        # fp16=True if torch.cuda.is_available() and not args.use_4bit else False,
    )

    # TraineråˆæœŸåŒ–
    print("\nğŸ“ Initializing LogitDistillationTrainer...")
    print(f"   Temperature: {args.distillation_logit_temperature}")
    print(f"   Alpha: {args.distillation_logit_alpha}")

    trainer = LogitDistillationTrainer(
        teacher_model=teacher_model,
        model=student_model,
        tokenizer=tokenizer,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=default_data_collator,  # labels ã‚’ä¸Šæ›¸ãã—ãªã„ã‚·ãƒ³ãƒ—ãƒ«ãª collator
        temperature=args.distillation_logit_temperature,
        alpha=args.distillation_logit_alpha,
    )

    # è¨“ç·´é–‹å§‹
    print("\nğŸš€ Starting training...")
    print("=" * 60 + "\n")

    trainer.train()

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    print("\nğŸ’¾ Saving model...")
    trainer.save_model(f"{args.output_dir}/{args.exper_name}/checkpoint-final")
    tokenizer.save_pretrained(f"{args.output_dir}/{args.exper_name}/checkpoint-final")

    print("\nâœ… Training complete!")
    print(f"   Model saved to: {args.output_dir}/{args.exper_name}/checkpoint-final")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Logit-based Knowledge Distillation")
    parser.add_argument("--exper_name", type=str, default="distill")
    parser.add_argument("--num_epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--learning_rate", type=float, default=5e-5, help="The initial learning rate for AdamW.")
    parser.add_argument("--optimizer", type=str, default="adamw_torch", help="The optimizer to use.")
    parser.add_argument("--logging_steps", type=int, default=50)
    parser.add_argument("--save_steps", type=int, default=1000)
    parser.add_argument("--save_total_limit", type=int, default=4)
    parser.add_argument("--output_dir", type=str, default="outputs")
    parser.add_argument("--teacher_model_name", type=str, default="Qwen/Qwen2-7B-Instruct")
    parser.add_argument("--student_model_name", type=str, default="Qwen/Qwen2-0.5B-Instruct")
    parser.add_argument("--tokenizer_max_length", type=int, default=512)
    parser.add_argument("--distillation_logit_temperature", type=float, default=1.5)
    parser.add_argument("--distillation_logit_alpha", type=float, default=0.5)
    parser.add_argument("--use_4bit", action="store_true", default=False)
    args = parser.parse_args()

    print("=" * 60)
    print("å®Ÿè¡Œæ¡ä»¶")
    print("=" * 60)
    for key, value in vars(args).items():
        print(f"{key}: {value}")
    print("=" * 60)

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(f"{args.output_dir}/{args.exper_name}", exist_ok=True)

    train(args)
