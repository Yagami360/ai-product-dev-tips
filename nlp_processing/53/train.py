import argparse
import os

import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DataCollatorForLanguageModeling, TrainingArguments

from trainer import LogitDistillationTrainer
from utils import print_gpu_memory, print_memory_summary, print_model_memory


def train(args):
    print("\n" + "=" * 60)
    print("Logit-based Knowledge Distillation")
    print("=" * 60)

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå˜èªãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼‰
    # æ•™å¸«ã®çŸ¥è­˜ã‚’æ­£ç¢ºã«è’¸ç•™ã™ã‚‹ãŸã‚ã«ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹
    print("\nğŸ“¥ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.teacher_model_name,
        trust_remote_code=True,
    )
    tokenizer.pad_token = tokenizer.eos_token

    # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ï¼ˆè’¸ç•™å…ƒãƒ¢ãƒ‡ãƒ«ï¼‰
    print(f"ğŸ“¥ Loading teacher model: {args.teacher_model_name}")
    if args.use_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        teacher_model = AutoModelForCausalLM.from_pretrained(
            args.teacher_model_name,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.float16,
            quantization_config=bnb_config,
        )
    else:
        teacher_model = AutoModelForCausalLM.from_pretrained(
            args.teacher_model_name,
            trust_remote_code=True,
            device_map="auto",
            # torch_dtype="auto",
            torch_dtype=torch.float16,
        )

    # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ï¼ˆè’¸ç•™å…ˆãƒ¢ãƒ‡ãƒ«ï¼‰
    print(f"ğŸ“¥ Loading student model: {args.student_model_name}")
    student_model = AutoModelForCausalLM.from_pretrained(
        args.student_model_name,
        trust_remote_code=True,
        device_map="auto",
        # torch_dtype="auto",
        torch_dtype=torch.float16,
        # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã¯ã€4bit é‡å­åŒ–åˆ©ç”¨ä¸å¯
        # load_in_4bit=True if args.use_4bit else False,
    )

    # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚ºã‚’æ•™å¸«ãƒ¢ãƒ‡ãƒ«ï¼ˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼‰ã«åˆã‚ã›ã‚‹
    # è’¸ç•™æ™‚ã«ã¯ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«ã‚¢ãƒ³ãƒãƒƒãƒãŒç”Ÿã˜ã‚‹ãŸã‚å¿…è¦
    # èªå½™ã‚µã‚¤ã‚ºï¼ˆVocabulary Sizeï¼‰: ãƒ¢ãƒ‡ãƒ«ãŒæ‰±ãˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå˜èªã®æœ€å°å˜ä½ï¼‰ã®ç¨®é¡ã®æ•°
    if student_model.config.vocab_size != len(tokenizer):
        print(f"   âš ï¸  Student-model vocab size mismatch! Resizing...")
        print(f"   Resizing student: {student_model.config.vocab_size} -> {len(tokenizer)}")
        student_model.resize_token_embeddings(len(tokenizer))

    # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚ºã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«åˆã‚ã›ã‚‹
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ã—ã¦æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€é€šå¸¸ã¯ä¸è¦
    if teacher_model.config.vocab_size != len(tokenizer):
        print(f"   âš ï¸  Teacher-model vocab size mismatch! Resizing...")
        print(f"   Resizing teacher: {teacher_model.config.vocab_size} -> {len(tokenizer)}")
        teacher_model.resize_token_embeddings(len(tokenizer))

    print("\nğŸ” Validating vocab sizes after resizing...")
    print(f"   Tokenizer:  {len(tokenizer)}")
    print(f"   Teacher Model:    {teacher_model.config.vocab_size}")
    print(f"   Student Model:    {student_model.config.vocab_size}")

    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¡¨ç¤º
    print_model_memory(teacher_model, f"Teacher Model: {args.teacher_model_name}")
    print_model_memory(student_model, f"Student Model: {args.student_model_name}")
    print_memory_summary(teacher_model, student_model, show_gpu=True)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
    # GSM8K: ç®—æ•°ã®å•é¡Œã¨è§£ç­”ãŒå«ã¾ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    print(f"\nğŸ“Š Loading dataset: GSM8K")
    dataset = load_dataset("gsm8k", "main", split="train[:]")
    print(f"âœ… Dataset loaded: {len(dataset)} samples")

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›: question ã¨ answer ã‚’çµåˆã—ã¦ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨ã™ã‚‹
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã« answer ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹ã“ã¨ã«ãªã‚‹ã®ã§ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã«æ¨è«–ã•ã›ã‚‹éš›ã«æ­£è§£ã‚’æ•™ãˆã¦ã„ã‚‹ã“ã¨ã«ãªã‚‹ã®ã§ã¯ï¼Ÿã¨ã„ã†ç–‘å•ãŒç”Ÿã˜ã‚‹ãŒã€
    # è’¸ç•™ã«ãŠã‘ã‚‹æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²ãŒã€ã€Œæ­£è§£ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã€ã§ã¯ãªãã€ã€Œæ­£è§£ã‚’çŸ¥ã£ã¦ã„ã‚‹çŠ¶æ…‹ã§ã®æ¨è«–ã®ã€æŒ¯ã‚‹èˆã„ã€ã‚’æä¾›ã™ã‚‹ã“ã¨ã€ã«ã‚ã‚‹ã®ã§å•é¡Œãªã—ã€‚
    dataset = dataset.map(lambda x: {"text": f"Q: {x['question']}\nA: {x['answer']}"})

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–: å˜èªãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›ã™ã‚‹
    print("   Tokenizing dataset...")

    def tokenize_function(examples):
        tokenized = tokenizer(examples["text"], truncation=True, max_length=256, padding="max_length", return_tensors=None)
        # labelsã‚’è¿½åŠ ï¼ˆLanguage Modelingç”¨ï¼‰
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized

    train_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)
    print(f"âœ… Dataset ready: {len(train_dataset)} samples")

    # Training Arguments
    print("\nâš™ï¸  Setting up training arguments...")
    training_args = TrainingArguments(
        output_dir=f"{args.output_dir}/{args.exper_name}",
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        logging_steps=args.logging_steps,
        logging_dir=f"{args.output_dir}/{args.exper_name}/logs",
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        dataloader_pin_memory=True if torch.cuda.is_available() else False,
        remove_unused_columns=False,
        fp16=True if torch.cuda.is_available() and not args.use_4bit else False,
    )

    # TraineråˆæœŸåŒ–
    print("\nğŸ“ Initializing LogitDistillationTrainer...")
    print(f"   Temperature: {args.distillation_logit_temperature}")
    print(f"   Alpha: {args.distillation_logit_alpha}")

    trainer = LogitDistillationTrainer(
        teacher_model=teacher_model,
        model=student_model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
        temperature=args.distillation_logit_temperature,
        alpha=args.distillation_logit_alpha,
    )

    # è¨“ç·´é–‹å§‹
    print("\nğŸš€ Starting training...")
    print("=" * 60 + "\n")

    trainer.train()

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    print("\nğŸ’¾ Saving model...")
    trainer.save_model(f"{args.output_dir}/{args.exper_name}/checkpoint-final")
    tokenizer.save_pretrained(f"{args.output_dir}/{args.exper_name}/checkpoint-final")

    print("\nâœ… Training complete!")
    print(f"   Model saved to: {args.output_dir}/{args.exper_name}/checkpoint-final")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Logit-based Knowledge Distillation")
    parser.add_argument("--exper_name", type=str, default="qwen2-7b-to-qwen2-0.5b-distill-20251119")
    parser.add_argument("--num_epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--logging_steps", type=int, default=100)
    parser.add_argument("--save_steps", type=int, default=500)
    parser.add_argument("--save_total_limit", type=int, default=4)
    parser.add_argument("--output_dir", type=str, default="outputs")
    parser.add_argument("--teacher_model_name", type=str, default="Qwen/Qwen2-7B-Instruct")
    # parser.add_argument("--teacher_model_name", type=str, default="Qwen/Qwen2-1.5B-Instruct")
    parser.add_argument("--student_model_name", type=str, default="Qwen/Qwen2-0.5B-Instruct")
    parser.add_argument("--distillation_logit_temperature", type=float, default=2.0)
    parser.add_argument("--distillation_logit_alpha", type=float, default=0.5)
    parser.add_argument("--use_4bit", action="store_true", default=True)
    args = parser.parse_args()

    print("=" * 60)
    print("å®Ÿè¡Œæ¡ä»¶")
    print("=" * 60)
    for key, value in vars(args).items():
        print(f"{key}: {value}")
    print("=" * 60)

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(f"{args.output_dir}/{args.exper_name}", exist_ok=True)

    train(args)
