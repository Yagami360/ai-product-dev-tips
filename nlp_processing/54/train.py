import argparse
import os
from pathlib import Path

import torch
from datasets import Dataset, load_dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer,
                          BitsAndBytesConfig)
from trl import GKDConfig, GKDTrainer

from utils import print_gpu_memory, print_memory_summary, print_model_memory


def prepare_dataset(dataset, tokenizer):
    """
    GKDTrainerç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™
    GKDTrainerã¯ "messages" å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’æœŸå¾…ã™ã‚‹
    """
    messages_data = []

    for example in dataset:
        messages = [
            {"role": "user", "content": f"Q: {example['question']}"},
            {"role": "assistant", "content": f"A: {example['answer']}"},
        ]
        messages_data.append(messages)

    return Dataset.from_dict({"messages": messages_data})


def train(args):
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå˜èªãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼‰
    # æ•™å¸«ã®çŸ¥è­˜ã‚’æ­£ç¢ºã«è’¸ç•™ã™ã‚‹ãŸã‚ã«ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹
    print("\nğŸ“¥ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.teacher_model_name,
        trust_remote_code=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
    if tokenizer.eos_token is None:
        tokenizer.add_special_tokens({"pad_token": "<|endoftext|>"})

    print(f"   Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    print(f"   EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")

    # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸ“¥ Loading teacher model: {args.teacher_model_name}")
    if args.use_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            # bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        teacher_model = AutoModelForCausalLM.from_pretrained(
            args.teacher_model_name,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype="auto",
            # torch_dtype=torch.float16,
            quantization_config=bnb_config,
        )
    else:
        teacher_model = AutoModelForCausalLM.from_pretrained(
            args.teacher_model_name,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype="auto",
            # torch_dtype=torch.float16,
        )

    # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ï¼ˆè’¸ç•™å…ˆãƒ¢ãƒ‡ãƒ«ï¼‰
    print(f"ğŸ“¥ Loading student model: {args.student_model_name}")
    student_model = AutoModelForCausalLM.from_pretrained(
        args.student_model_name,
        trust_remote_code=True,
        device_map="auto",
        torch_dtype="auto",
        # torch_dtype=torch.float16,
        # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã¯ã€4bit é‡å­åŒ–åˆ©ç”¨ä¸å¯
        # load_in_4bit=True if args.use_4bit else False,
    )

    # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚ºã‚’æ•™å¸«ãƒ¢ãƒ‡ãƒ«ï¼ˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼‰ã«åˆã‚ã›ã‚‹
    # è’¸ç•™æ™‚ã«ã¯ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«ã‚¢ãƒ³ãƒãƒƒãƒãŒç”Ÿã˜ã‚‹ãŸã‚å¿…è¦
    # èªå½™ã‚µã‚¤ã‚ºï¼ˆVocabulary Sizeï¼‰: ãƒ¢ãƒ‡ãƒ«ãŒæ‰±ãˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå˜èªã®æœ€å°å˜ä½ï¼‰ã®ç¨®é¡ã®æ•°
    if len(tokenizer) != student_model.config.vocab_size:
        print(
            f"âš ï¸  ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚ºã‚’èª¿æ•´: {student_model.config.vocab_size} -> {len(tokenizer)}"
        )
        student_model.resize_token_embeddings(len(tokenizer))

    # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚ºã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«åˆã‚ã›ã‚‹
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ã—ã¦æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€é€šå¸¸ã¯ä¸è¦
    if len(tokenizer) != teacher_model.config.vocab_size:
        print(
            f"âš ï¸  æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚ºã‚’èª¿æ•´: {teacher_model.config.vocab_size} -> {len(tokenizer)}"
        )
        teacher_model.resize_token_embeddings(len(tokenizer))

    # ãƒ¢ãƒ‡ãƒ«ã® pad_token_id ã¨ eos_token_id ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ä¸€è‡´ã•ã›ã‚‹
    teacher_model.config.pad_token_id = tokenizer.pad_token_id
    teacher_model.config.eos_token_id = tokenizer.eos_token_id
    student_model.config.pad_token_id = tokenizer.pad_token_id
    student_model.config.eos_token_id = tokenizer.eos_token_id

    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¡¨ç¤º
    print_model_memory(teacher_model, f"Teacher Model: {args.teacher_model_name}")
    print_model_memory(student_model, f"Student Model: {args.student_model_name}")
    print_memory_summary(teacher_model, student_model, show_gpu=True)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {args.dataset_name}")
    dataset = load_dataset(args.dataset_name, args.dataset_config, split="train")
    train_dataset = prepare_dataset(dataset, tokenizer)
    eval_dataset = train_dataset.select(range(min(100, len(train_dataset))))
    print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} samples")
    print(f"âœ… è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {len(eval_dataset)} samples")

    # GKDTrainer ã®è¨­å®š
    training_args = GKDConfig(
        output_dir=f"{args.output_dir}/{args.exper_name}",
        logging_dir=f"{args.output_dir}/{args.exper_name}/logs",
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        eval_strategy="steps",
        eval_steps=args.save_steps,
        warmup_steps=100,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        gradient_checkpointing=True,
        # fp16=True if torch.cuda.is_available() and not args.use_4bit else False,
        # bf16=True if torch.cuda.is_available() and args.use_4bit else False,
        optim=args.optimizer,
        temperature=args.temperature,
        lmbda=args.lmbda,       # ç”Ÿå¾’ãƒ‡ãƒ¼ã‚¿å‰²åˆï¼ˆ0.0-1.0ï¼‰
        beta=args.beta,         # JSDè£œé–“ä¿‚æ•°ï¼ˆ0.0=KL, 1.0=é€†KLï¼‰
        max_new_tokens=args.max_new_tokens,
        seq_kd=args.seq_kd,     # Sequence-Level KD
        disable_dropout=True,
        report_to=["tensorboard"],
        push_to_hub=False,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
    )
    trainer = GKDTrainer(
        model=student_model,
        teacher_model=teacher_model,
        args=training_args,
        processing_class=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    # å­¦ç¿’ã®å®Ÿè¡Œ
    print("\nğŸš€ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    print("=" * 60)
    trainer.train()

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    print("\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
    trainer.save_model(f"{args.output_dir}/{args.exper_name}/checkpoint-final")
    tokenizer.save_pretrained(f"{args.output_dir}/{args.exper_name}/checkpoint-final")
    print(f"\nâœ… è¨“ç·´ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"   ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {args.output_dir}/{args.exper_name}/checkpoint-final")
    print("=" * 60)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GKDã‚’ä½¿ç”¨ã—ãŸçŸ¥è­˜è’¸ç•™")
    parser.add_argument("--exper_name", type=str, required=True, help="å®Ÿé¨“å")
    parser.add_argument("--output_dir", type=str, default="outputs", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--teacher_model_name", type=str, required=True, help="æ•™å¸«ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--student_model_name", type=str, required=True, help="ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument(
        "--dataset_name", type=str, default="openai/gsm8k", help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå"
    )
    parser.add_argument("--dataset_config", type=str, default="main", help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š")
    parser.add_argument("--num_epochs", type=int, default=5, help="ã‚¨ãƒãƒƒã‚¯æ•°")
    parser.add_argument("--batch_size", type=int, default=2, help="ãƒãƒƒãƒã‚µã‚¤ã‚º")
    parser.add_argument(
        "--gradient_accumulation_steps", type=int, default=4, help="å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°"
    )
    parser.add_argument("--learning_rate", type=float, default=5e-5, help="å­¦ç¿’ç‡")
    parser.add_argument("--optimizer", type=str, default="adamw_torch", help="ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶")
    parser.add_argument("--logging_steps", type=int, default=50, help="ãƒ­ã‚°å‡ºåŠ›é–“éš”")
    parser.add_argument("--save_steps", type=int, default=500, help="ä¿å­˜é–“éš”")
    parser.add_argument(
        "--save_total_limit", type=int, default=3, help="ä¿å­˜ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ•°ã®ä¸Šé™"
    )
    parser.add_argument("--temperature", type=float, default=0.9, help="ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦")
    parser.add_argument("--lmbda", type=float, default=0.7, help="ç”Ÿå¾’ãƒ‡ãƒ¼ã‚¿å‰²åˆ (0.0-1.0)")
    parser.add_argument(
        "--beta", type=float, default=0.5, help="JSDè£œé–“ä¿‚æ•° (0.0=KL, 1.0=é€†KL)"
    )
    parser.add_argument("--max_new_tokens", type=int, default=512, help="ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--seq_kd", action="store_true", help="Sequence-Level KDã‚’ä½¿ç”¨")
    parser.add_argument(
        "--use_4bit", action="store_true", default=False, help="4bité‡å­åŒ–ã‚’ä½¿ç”¨"
    )
    args = parser.parse_args()

    print("=" * 60)
    print("å®Ÿè¡Œæ¡ä»¶")
    print("=" * 60)
    for key, value in vars(args).items():
        print(f"{key}: {value}")
    print("=" * 60)

    os.makedirs(f"{args.output_dir}/{args.exper_name}", exist_ok=True)
    train(args)
