"""
GKD (Generalized Knowledge Distillation) ã‚’ä½¿ç”¨ã—ãŸçŸ¥è­˜è’¸ç•™ã®è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
https://huggingface.co/docs/trl/main/gkd_trainer

GKDã®ä¸»ãªåˆ©ç‚¹:
1. è¨“ç·´ã¨æ¨è«–ã®åˆ†å¸ƒãƒŸã‚¹ãƒãƒƒãƒã‚’è§£æ±º
2. ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ãŒè‡ªå·±ç”Ÿæˆã—ãŸå‡ºåŠ›ã«å¯¾ã—ã¦æ•™å¸«ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å—ã‘ã‚‹
3. æŸ”è»Ÿãªæå¤±é–¢æ•°ã®é¸æŠãŒå¯èƒ½
"""

import argparse
from pathlib import Path

from datasets import Dataset, load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import GKDConfig, GKDTrainer


def prepare_dataset_for_gkd(dataset, tokenizer):
    """
    GKDTrainerç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™
    GKDTrainerã¯ "messages" å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’æœŸå¾…ã™ã‚‹
    """
    messages_data = []

    for example in dataset:
        messages = [
            {"role": "user", "content": f"Q: {example['question']}"},
            {"role": "assistant", "content": f"A: {example['answer']}"},
        ]
        messages_data.append(messages)

    return Dataset.from_dict({"messages": messages_data})


def train(args):
    """GKDã‚’ä½¿ç”¨ã—ãŸçŸ¥è­˜è’¸ç•™ã®è¨“ç·´"""

    print("=" * 60)
    print("GKD (Generalized Knowledge Distillation) Training")
    print("=" * 60)
    print(f"å®Ÿé¨“å: {args.exper_name}")
    print(f"æ•™å¸«ãƒ¢ãƒ‡ãƒ«: {args.teacher_model_name}")
    print(f"ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«: {args.student_model_name}")
    print(f"Lambda (ç”Ÿå¾’ãƒ‡ãƒ¼ã‚¿å‰²åˆ): {args.lmbda}")
    print(f"Beta (JSDè£œé–“ä¿‚æ•°): {args.beta}")
    print(f"Temperature: {args.temperature}")
    print(f"Sequence-Level KD: {args.seq_kd}")
    print("=" * 60)
    print()

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
    output_dir = Path(args.output_dir) / args.exper_name
    output_dir.mkdir(parents=True, exist_ok=True)

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
    print("ğŸ“¥ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.teacher_model_name, trust_remote_code=True
    )

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": "<|pad|>"})

    # é‡å­åŒ–è¨­å®šï¼ˆ4bitä½¿ç”¨æ™‚ï¼‰
    quantization_config = None
    if args.use_4bit:
        # BitsAndBytesConfigã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ä½¿ç”¨
        import torch
        from transformers import BitsAndBytesConfig

        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )

    # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸ“¥ æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {args.teacher_model_name}")
    if args.use_4bit:
        teacher_model = AutoModelForCausalLM.from_pretrained(
            args.teacher_model_name,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
        )
    else:
        import torch

        teacher_model = AutoModelForCausalLM.from_pretrained(
            args.teacher_model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )

    # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸ“¥ ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {args.student_model_name}")
    # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã¯å¸¸ã«bf16ã§èª­ã¿è¾¼ã‚€
    try:
        import torch

        student_model = AutoModelForCausalLM.from_pretrained(
            args.student_model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
    except ImportError:
        # torchãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ããªã„å ´åˆã¯é€šå¸¸ã®èª­ã¿è¾¼ã¿
        student_model = AutoModelForCausalLM.from_pretrained(
            args.student_model_name,
            device_map="auto",
            trust_remote_code=True,
        )

    # èªå½™ã‚µã‚¤ã‚ºã®èª¿æ•´ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    if len(tokenizer) > student_model.config.vocab_size:
        print(
            f"âš ï¸  ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚ºã‚’èª¿æ•´: {student_model.config.vocab_size} -> {len(tokenizer)}"
        )
        student_model.resize_token_embeddings(len(tokenizer))

    if len(tokenizer) > teacher_model.config.vocab_size:
        print(
            f"âš ï¸  æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚ºã‚’èª¿æ•´: {teacher_model.config.vocab_size} -> {len(tokenizer)}"
        )
        teacher_model.resize_token_embeddings(len(tokenizer))

    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    try:
        from utils import print_memory_summary

        print_memory_summary(teacher_model, student_model, show_gpu=True)
    except ImportError:
        # utilsãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        print("âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆutils.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {args.dataset_name}")
    dataset = load_dataset(args.dataset_name, args.dataset_config, split="train")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’GKDå½¢å¼ã«å¤‰æ›
    train_dataset = prepare_dataset_for_gkd(dataset, tokenizer)

    # è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’ä½¿ç”¨ï¼‰
    eval_dataset = train_dataset.select(range(min(100, len(train_dataset))))

    print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} samples")
    print(f"âœ… è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {len(eval_dataset)} samples")

    # GKDè¨­å®š
    training_args = GKDConfig(
        output_dir=str(output_dir),
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        evaluation_strategy="steps",
        eval_steps=args.save_steps,
        warmup_steps=100,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        gradient_checkpointing=True,
        fp16=False,
        bf16=True,
        optim=args.optimizer,
        # GKDç‰¹æœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        temperature=args.temperature,
        lmbda=args.lmbda,  # ç”Ÿå¾’ãƒ‡ãƒ¼ã‚¿å‰²åˆï¼ˆ0.0-1.0ï¼‰
        beta=args.beta,  # JSDè£œé–“ä¿‚æ•°ï¼ˆ0.0=KL, 1.0=é€†KLï¼‰
        max_new_tokens=args.max_new_tokens,
        seq_kd=args.seq_kd,  # Sequence-Level KD
        disable_dropout=True,
        report_to=["tensorboard"],
        logging_dir=str(output_dir / "logs"),
        push_to_hub=False,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
    )

    # GKDTrainerã®åˆæœŸåŒ–
    print("\nğŸ“ GKDTrainerã‚’åˆæœŸåŒ–ä¸­...")
    trainer = GKDTrainer(
        model=student_model,
        teacher_model=teacher_model,
        args=training_args,
        processing_class=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    # è¨“ç·´ã®å®Ÿè¡Œ
    print("\nğŸš€ è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...")
    print("=" * 60)
    trainer.train()

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    print("\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
    trainer.save_model(str(output_dir / "checkpoint-final"))
    tokenizer.save_pretrained(str(output_dir / "checkpoint-final"))

    print(f"\nâœ… è¨“ç·´ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"   ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {output_dir / 'checkpoint-final'}")
    print("=" * 60)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GKDã‚’ä½¿ç”¨ã—ãŸçŸ¥è­˜è’¸ç•™")
    parser.add_argument("--exper_name", type=str, required=True, help="å®Ÿé¨“å")
    parser.add_argument("--output_dir", type=str, default="outputs", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--teacher_model_name", type=str, required=True, help="æ•™å¸«ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--student_model_name", type=str, required=True, help="ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument(
        "--dataset_name", type=str, default="openai/gsm8k", help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå"
    )
    parser.add_argument("--dataset_config", type=str, default="main", help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š")
    parser.add_argument("--num_epochs", type=int, default=3, help="ã‚¨ãƒãƒƒã‚¯æ•°")
    parser.add_argument("--batch_size", type=int, default=2, help="ãƒãƒƒãƒã‚µã‚¤ã‚º")
    parser.add_argument(
        "--gradient_accumulation_steps", type=int, default=4, help="å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°"
    )
    parser.add_argument("--learning_rate", type=float, default=5e-5, help="å­¦ç¿’ç‡")
    parser.add_argument("--optimizer", type=str, default="adamw_torch", help="ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶")
    parser.add_argument("--logging_steps", type=int, default=50, help="ãƒ­ã‚°å‡ºåŠ›é–“éš”")
    parser.add_argument("--save_steps", type=int, default=500, help="ä¿å­˜é–“éš”")
    parser.add_argument(
        "--save_total_limit", type=int, default=3, help="ä¿å­˜ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ•°ã®ä¸Šé™"
    )
    parser.add_argument("--temperature", type=float, default=0.9, help="ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦")
    parser.add_argument("--lmbda", type=float, default=0.7, help="ç”Ÿå¾’ãƒ‡ãƒ¼ã‚¿å‰²åˆ (0.0-1.0)")
    parser.add_argument(
        "--beta", type=float, default=0.5, help="JSDè£œé–“ä¿‚æ•° (0.0=KL, 1.0=é€†KL)"
    )
    parser.add_argument("--max_new_tokens", type=int, default=128, help="ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--seq_kd", action="store_true", help="Sequence-Level KDã‚’ä½¿ç”¨")
    parser.add_argument(
        "--use_4bit", action="store_true", default=True, help="4bité‡å­åŒ–ã‚’ä½¿ç”¨"
    )

    args = parser.parse_args()
    train(args)
