"""
GKDã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import argparse
import json
from pathlib import Path

from datasets import load_dataset
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer


def predict(args):
    """ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã¨è©•ä¾¡"""

    print("=" * 60)
    print("Model Inference and Evaluation")
    print("=" * 60)
    print(f"Model: {args.model_name}")
    print(f"Dataset: {args.dataset_name}")
    print(f"Num samples: {args.num_samples}")
    print(f"Batch size: {args.batch_size}")
    print("=" * 60)
    print()

    # ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®š
    if "/" in args.model_name:
        # Hugging Faceãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
        model_basename = args.model_name.replace("/", "_")
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
        model_basename = Path(args.model_name).name

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
    print("ğŸ“¥ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    if "checkpoint" in args.model_name or Path(args.model_name).exists():
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å ´åˆ
        tokenizer = AutoTokenizer.from_pretrained(
            args.model_name, trust_remote_code=True
        )
    else:
        # Hugging Faceãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
        tokenizer = AutoTokenizer.from_pretrained(
            args.model_name, trust_remote_code=True
        )

    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": "<|pad|>"})

    # æ¨è«–æ™‚ã¯å·¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    tokenizer.padding_side = "left"

    print(f"   Vocab size: {len(tokenizer)}")
    print(f"   Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    print(f"   Padding side: {tokenizer.padding_side}")

    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    print(f"\nğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {args.model_name}")
    try:
        import torch

        model = AutoModelForCausalLM.from_pretrained(
            args.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        model.eval()
    except ImportError:
        # torchãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ããªã„å ´åˆã¯é€šå¸¸ã®èª­ã¿è¾¼ã¿
        model = AutoModelForCausalLM.from_pretrained(
            args.model_name,
            device_map="auto",
            trust_remote_code=True,
        )

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {args.dataset_name}")
    dataset = load_dataset(args.dataset_name, args.dataset_config, split="test")

    # ã‚µãƒ³ãƒ—ãƒ«æ•°ã®åˆ¶é™
    if args.num_samples > 0:
        dataset = dataset.select(range(min(args.num_samples, len(dataset))))

    print(f"âœ… è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset)}")

    # æ¨è«–ã®å®Ÿè¡Œ
    print(f"\nğŸš€ æ¨è«–ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º: {args.batch_size}ï¼‰...")

    samples = []
    correct_count = 0

    for batch_start in tqdm(range(0, len(dataset), args.batch_size), desc="æ¨è«–ä¸­"):
        batch_end = min(batch_start + args.batch_size, len(dataset))
        batch = dataset[batch_start:batch_end]

        questions = batch["question"]
        answers_gt = batch["answer"]

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        prompts = [f"Q: {q}\nA:" for q in questions]

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512,
        ).to(model.device)

        # ç”Ÿæˆ
        try:
            import torch

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    num_return_sequences=1,
                    pad_token_id=tokenizer.pad_token_id,
                    do_sample=False,
                    temperature=0.1,
                )
        except ImportError:
            # torchãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ããªã„å ´åˆ
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                num_return_sequences=1,
                pad_token_id=tokenizer.pad_token_id,
                do_sample=False,
                temperature=0.1,
            )

        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        # çµæœã®å‡¦ç†
        for question, answer_gt, generated_text in zip(
            questions, answers_gt, generated_texts
        ):
            # å›ç­”éƒ¨åˆ†ã®æŠ½å‡º
            if "\nA:" in generated_text:
                answer_pred = generated_text.split("\nA:")[-1].strip()
            else:
                answer_pred = generated_text.strip()

            # æ­£è§£åˆ¤å®šï¼ˆç°¡æ˜“çš„ãªæ–‡å­—åˆ—æ¯”è¼ƒï¼‰
            is_correct = answer_gt.lower().strip() in answer_pred.lower()
            if is_correct:
                correct_count += 1

            samples.append(
                {
                    "question": question,
                    "answer_gt": answer_gt,
                    "answer_pred": answer_pred,
                    "is_correct": is_correct,
                }
            )

    # ç²¾åº¦ã®è¨ˆç®—
    accuracy = correct_count / len(samples) * 100 if samples else 0

    # çµæœã®è¡¨ç¤º
    print(f"\nğŸ“Š è©•ä¾¡çµæœ:")
    print(f"   æ­£è§£æ•°: {correct_count}/{len(samples)}")
    print(f"   ç²¾åº¦: {accuracy:.2f}%")

    # çµæœã®ä¿å­˜
    # çµ±è¨ˆæƒ…å ±
    stats_filename = f"eval_stats_{model_basename}_n{len(samples)}.txt"
    stats_filepath = output_dir / stats_filename
    with open(stats_filepath, "w", encoding="utf-8") as f:
        f.write(f"Model: {args.model_name}\n")
        f.write(f"Correct: {correct_count}/{len(samples)}\n")
        f.write(f"Accuracy: {accuracy:.2f}%\n")
    print(f"\n   çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜: {stats_filepath}")

    # ã‚µãƒ³ãƒ—ãƒ«è©³ç´°
    samples_filename = f"eval_samples_{model_basename}_n{len(samples)}.json"
    samples_filepath = output_dir / samples_filename
    with open(samples_filepath, "w", encoding="utf-8") as f:
        json.dump(samples, f, ensure_ascii=False, indent=2)
    print(f"   ã‚µãƒ³ãƒ—ãƒ«è©³ç´°ã‚’ä¿å­˜: {samples_filepath}")

    # ã‚µãƒ³ãƒ—ãƒ«ä¾‹ã®è¡¨ç¤º
    print("\n--- ã‚µãƒ³ãƒ—ãƒ«ä¾‹ ---")
    for i, ex in enumerate(samples[: args.num_example_to_show]):
        print(f"\nä¾‹ {i+1}:")
        print(f"  Q: {ex['question']}")
        print(f"  A (æ­£è§£): {ex['answer_gt']}")
        print(f"  A (äºˆæ¸¬): {ex['answer_pred']}")
        print(f"  æ­£è§£: {'âœ“' if ex['is_correct'] else 'âœ—'}")
    print("-" * 60)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GKDãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã¨è©•ä¾¡")

    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    parser.add_argument("--model_name", type=str, required=True, help="è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åã¾ãŸã¯ãƒ‘ã‚¹")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
    parser.add_argument(
        "--dataset_name", type=str, default="openai/gsm8k", help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå"
    )
    parser.add_argument("--dataset_config", type=str, default="main", help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š")

    # æ¨è«–è¨­å®š
    parser.add_argument("--batch_size", type=int, default=4, help="ãƒãƒƒãƒã‚µã‚¤ã‚º")
    parser.add_argument("--num_samples", type=int, default=100, help="è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ0ã§å…¨ã¦ï¼‰")
    parser.add_argument(
        "--num_example_to_show", type=int, default=10, help="è¡¨ç¤ºã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ä¾‹ã®æ•°"
    )

    # å‡ºåŠ›è¨­å®š
    parser.add_argument(
        "--output_dir", type=str, default="outputs/evaluation", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )

    args = parser.parse_args()
    predict(args)
